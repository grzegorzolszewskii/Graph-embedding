co kopiowac:
tworzenie modelu, wpisywanie danych - napisać samemu
prz. hiperb., gradient Riemannowski mozna przekopiować(optimizer oni robia zle)

sktruktura programu:
1) plik embed.py - tworzy model, steruje programem, pętla trenująca model

	plik embed.py:

	a) tworzenie modelu - nie wrzucam grafu do modelu, jego parametrami są min. wymiary
	model przetrzymuje wspolrzedne wierzcholkow
	[np. graf ma 10 wierzcholkow zanurzam w R^4 to macierz jest 10x4]

	b) tworzy sie model, potem przekazuje sie go do modulu treningowego "train"

	model = model.to(device)

2) plik hype/train.py
	
	linia 39 glowna petla idaca po epokach
	
	od linii 50 jest ładowanie danych. Napisać po swojemu, nie musi byc macierz,
	niech wierzchołek ma liste wierzcholkow sąsiadujących.
	
	targets linia 51 nas nie interesuje - wektor samych zer
	
	inputs - mamy 100 wierzch. nie chcemy ich dawac od razu:
	za pomocą batch dzielimy 100 wierzch na 10 paczek po 10 wierzcholkow
	Dla każdego z 10 losuje 51 - 1 połączony, 50 niepołączony
	
	inputs to macierz 10x52 (pytorch tensor), pierwszy wiersz i pierwsza kol to numer wierzchołka
	połączonego z batch, 1 kolumna to te wierzch. ktore bierzemy pod uwagę
	2 kolumna to połączone, kolejne 50 kolumn to niepołączone
	
	load_iter - funkcja losująca wierzchołki, macierz inputs; napisac samemu
	
	model to obiekt klasy energy_function
	
	wrzucamy inputs do modelu, linia 64 to jest "zanurzenie"
	preds - macierz 10x52xliczba_wymiarow dla R^2 to bedzie 2, czyli mamy
	w jednej macierzy 520 wierzcholkow o 3 roznych kategoriach
	
	preds = model(inputs) - wywołuje funkcję forward
	model bierze inputs i wyrzuca preds
	
	linijka 65 liczy funkcje kosztu, loss.backward() rozniczkuje funkcje w punkcie
	w jakim chce, liczy gradient wzgledem wszystkich wsp. wszystkich wierzch.
	67 linijka doda -gradient*learning_rate do wsp. wiercholkow
	w jednej epoce wierzch. moze byc ruszany kilkukrotnie
	
3) plik energy_function.py - wazne init i forward
	ostatnia klasa niepotrzebna, druga potrzebna, moze uda sie polaczyc pierwszą z drugą
	lt to będzie model - w środku klasy model nazywa się lt
	można napisać self.lt = Embedding(...); Embedding to typ modelu, który robi zanurzanie
	naszym modelem jest jedna warstwa Embedding - jest to macierz n x dim
	
	funkcja forwad:
		e - macierz 10x52xliczba_wymiarow; e to będzie to preds
		
		23-24 linijki nie istnieją w R^n euklid.
		
		s - pierwsza kolumna macierzy e, 10 wierzch. na ktore patrzymy - potem musimy ją rozszerzyć żeby porównać z o.
		o - reszta wierzch
		
		funkcja zwraca energię(s,o), czyli macierz odległości postaci 10x51 określająca odległości
		między wierzch. początkowymi a pozostałymi 51. 1 kolumna to odl. miedzy pocz. a polaczonymi,
		reszta to odl. miedzy pocz. a niepołączonymi.
		
	loss wykonuje funkcję crossentropy na macierzy odległości - połączone mają być małe, niepoł. mają być długie
	

4) manifoldy - bede robic euclidean i lorentz
	trzeba zdefiniować jak liczymy odległość
	
	linijka 19 jakos losowo zanurza wierzchołki

	
	

n) optimizer - coś w modelu, funkcja ustalająca wagi